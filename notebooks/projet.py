# -*- coding: utf-8 -*-
"""version02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10oUj0q1sjR-x_mnpOdbABs3BqUsqxyxn
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.preprocessing import StandardScaler, MinMaxScaler

pd.set_option('display.max_columns', None)

print("Pandas version: ", pd.__version__)
print("Numpy version: ", np.__version__)
print("Scikit-learn version: ", sklearn.__version__)

"""Chargement des données :"""

from google.colab import files

# Télécharger un fichier
uploaded = files.upload()

"""# Partie 1 : Enrichissement du Dataset"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Charger les données
def load_data(file_path):
    data = pd.read_csv(file_path, delim_whitespace=True, header=None)
    data.columns = ['engine_id', 'cycle', 'setting_1', 'setting_2', 'setting_3', 'sensor_1',
                     'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7',
                     'sensor_8', 'sensor_9', 'sensor_10', 'sensor_11', 'sensor_12',
                     'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16', 'sensor_17',
                     'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21']
    return data

# Normaliser les données
def normalize_data(data, sensor_columns):
    scaler = StandardScaler()
    data[sensor_columns] = scaler.fit_transform(data[sensor_columns])
    return data, scaler

# Méthode pour ajouter du bruit aléatoire
def add_random_noise(data, sensor_columns, noise_factor=0.05):
    noisy_data = data.copy()
    for col in sensor_columns:
        noisy_data[col] += noise_factor * np.random.normal(size=len(data))
    return noisy_data

# Méthode pour simuler des défaillances de capteur
def simulate_sensor_failure(data, sensor_columns):
    failed_data = data.copy()
    failed_sensor = np.random.choice(sensor_columns)
    failed_data[failed_sensor] = 0  # Défaillance du capteur
    return failed_data

# Méthode pour enrichir le dataset
def enrich_dataset(data, sensor_columns):
    enriched_data = []

    # Ajouter des versions avec bruit
    for _ in range(3):
        noisy_data = add_random_noise(data, sensor_columns)
        enriched_data.append(noisy_data)

    # Ajouter des versions avec défaillances de capteurs
    for _ in range(2):
        failed_data = simulate_sensor_failure(data, sensor_columns)
        enriched_data.append(failed_data)

    # Ajouter les données originales
    enriched_data.append(data)

    # Utiliser pd.concat au lieu de append()
    enriched_data = pd.concat(enriched_data, ignore_index=True)

    return enriched_data

# Charger et enrichir les données
sensor_columns = [f'sensor_{i}' for i in range(1, 22)]
train_data = load_data('train_FD001.txt')
train_data, scaler = normalize_data(train_data, sensor_columns)
enriched_train_data = enrich_dataset(train_data, sensor_columns)

# Enregistrer le dataset enrichi
enriched_train_data.to_csv('enriched_train_FD001.csv', index=False)

from google.colab import files

# Télécharger le fichier CSV
files.download('enriched_train_FD001.csv')

import pandas as pd

# Lire le fichier téléchargé
train_data = pd.read_csv("train_FD001.txt", sep="\s+", header=None)

# Afficher la forme du DataFrame
print(train_data.shape)

train_data.head()

"""# Exploration des données:"""

import numpy as np
np.unique(train_data[0])

"""Il y a donc 100 moteurs,


maintenant après combien de cycles chaque moteur tombe-t-il en panne ?

calcule le nombre de cycles que chaque moteur a traversé jusqu'à sa panne.
"""

num_cycles_to_failure = train_data.groupby(0)[1].count()
num_cycles_to_failure.values

"""# Comment trouver les valeurs RUL d’entraînement ?"""

# Modèle de dégradation linéaire
engine_1_linear_degradation_model = np.arange(192-1, -1, -1)

# Modèle de dégradation linéaire par morceaux
early_rul = 125
engine_1_piecewise_linear_degradation_model = np.append(early_rul*np.ones(shape = (192-early_rul,)),
                                                        np.arange(early_rul-1, -1, -1))

print("Linear degradation model shape (for engine 1): ", engine_1_linear_degradation_model.shape)
print("Piecewise linear degradation model shape (for engine 1): ", engine_1_piecewise_linear_degradation_model.shape)

"""

 Cela indique que les deux modèles ont la même forme, ce qui signifie qu'ils couvrent le même nombre de cycles avant la panne du moteur.


---

"""

import matplotlib.pyplot as plt
plt.figure(figsize = (12, 6))
plt.subplot(121)
plt.plot(engine_1_linear_degradation_model)
plt.title("Linear degradation model (engine 1)")
plt.subplot(122)
plt.plot(engine_1_piecewise_linear_degradation_model)
plt.title("Piecewise linear degradation plot (engine 1)")
plt.show()

def process_targets(data_length, early_rul = None):
    """
    Takes datalength (i.e., total number of cycles for each engine) and early_rul as input and
    creates target rul.

    Arguments:
        data_length: (scaler) Number of cycles for each engine
        early_rul: (scaler) Early RUL value to be set. When set to `None`, linear degradation
                   curve is used. (deafult: None)

    Returns:
        target_array: (1D float array) Target array
    """
    if early_rul == None:
        return np.arange(data_length-1, -1, -1)
    else:
        early_rul_duration = data_length - early_rul
        if early_rul_duration <= 0:    # This condition is needed when early rul is larger than data_length of an engine
            target_array = np.arange(data_length-1, -1, -1)
            return target_array
        else:
            target_array = np.append(early_rul*np.ones(shape = (early_rul_duration,)), np.arange(early_rul-1, -1, -1))
            return target_array

"""On va essayier maintenant de visualiser les modèles de dégradation linéaire

et linéaire par morceaux pour un moteur particulier (moteur 7) en utilisant la fonction process_targets
"""

engine_7_linear_degradation_rul = process_targets(259, early_rul = None)
engine_7_piecewise_linear_degradation_rul = process_targets(259, early_rul = 125)
plt.figure(figsize = (12, 6))
plt.subplot(121)
plt.plot(engine_7_linear_degradation_rul)
plt.title("Linear degradation model (engine 7)")
plt.subplot(122)
plt.plot(engine_7_piecewise_linear_degradation_rul)
plt.title("Piecewise linear degradation plot (engine 7)")
plt.show()

"""#Analyser les données de test

`Chargement des données de test`
"""

test_data = pd.read_csv("test_FD001.txt", sep = "\s+", header = None)
test_data.shape

test_data.head()

"""
### Combien de moteurs y a-t-il dans l’ensemble de test ?"""

np.unique(test_data[0])

"""nombre de cycles de données sont disponibles pour chaque moteur dans l'ensemble de test"""

test_data.groupby(0)[1].count().values

"""En utilisant les données disponibles, pour prédire le RUL pour chaque moteur.

## Chargement des vraies valeurs RUL
"""

true_rul = pd.read_csv('RUL_FD001.txt', sep = '\s+', header = None)
true_rul[0].values

"""## visualiser la distribution des valeurs des capteurs."""

plt.figure(figsize = (15, 21))
for i in np.arange(5, 26):        # column 5 in python means actual 6th column
    temp = train_data.iloc[:, i]
    plt.subplot(7,3, i-4)
    plt.boxplot(temp)
    plt.title("Senor: "+ str(i-4))
plt.show()

"""Cela vous donne un aperçu des distributions des valeurs des capteurs, ce qui est utile pour détecter les capteurs qui peuvent être non informatifs ou qui présentent des anomalies."""

train_data[10].value_counts()

import seaborn as sns
plt.figure(figsize = (15, 21))
for i,j in enumerate([6, 7, 8, 11, 12, 13, 15, 16, 17, 18, 19, 21, 24, 25]):
    temp = train_data.iloc[:, j]
    plt.subplot(7,3, i+1)
    sns.kdeplot(temp, legend = False)
    plt.title("Column: "+ str(j+1))
plt.show()

"""Cette visualisation permet d'identifier les colonnes qui ont une distribution intéressante pour la modélisation.

---
## comparer la distribution des données d'entraînement et des données de test à l'aide de graphiques de densité
"""

plt.figure(figsize = (15, 21))
for i,j in enumerate([6, 7, 8, 11, 12, 13, 15, 16, 17, 18, 19, 21, 24, 25]):
    temp_train = train_data.iloc[:, j]
    temp_test = test_data.iloc[:, j]
    plt.subplot(7,3, i+1)
    sns.kdeplot(temp_train, legend = False, color = "blue", label = "Train")
    sns.kdeplot(temp_test, legend = False, color = "red", label = "Test")
    plt.title("Column: "+ str(j+1))
    plt.legend()
plt.show()

"""les données de test ont une distribution similaire à celle des données d’entraînement. Donc,il se généralisera bien.

### Cette fonction génère des lots de données et des cibles selon les paramètres de la fenêtre et du décalage, en ajustant les dimensions en conséquence.
"""

def process_input_data_with_targets(input_data, target_data = None, window_length = 1, shift = 1):
    """
    Depending on values of window_length and shift, this function generates batchs of data and targets
    from `input_data` and `target_data`. `target_data` is optional. If no `target_data` is given, i.e.,
    `target_data = None`, no `output_targets` are generated. The following formula is used to determine
    number of batches. Please note that, here we use the term `batch` in a different sense than its usual
    meaning in deep learning.

    Number of batches = int(np.floor((len(input_data) - window_length)/shift)) + 1

    **We don't check input dimensions using exception handling. So readers should be careful while using these
    functions. If input data are not of desired dimension, either error occurs or something undesirable is
    produced as output.**

    Arguments:
        input_data: (Must be 2D array) input data to function
        target_data (optional): (Must be 1D array) Input rul values (default: None)
        window_length: (scalar) window length of data (default: 1)
        shift: (scalar) Distance by which the window moves for next batch. This is closely related to overlap
               between data. For example, if window length is 30 and shift is 1, there is an overlap of
               29 data points between two consecutive batches (default: 1)

    Returns:
        output_data: (2D float array) Output data that would be passed to machine learning model
        output_targets (optional): (1D float array) Output targets. Only generated if `target_data` is
                                   passed as argument.
    """
    num_batches = int(np.floor((len(input_data) - window_length)/shift)) + 1
    num_features = input_data.shape[1]
    output_data = np.repeat(np.nan, repeats = num_batches * window_length * num_features).reshape(num_batches, window_length,
                                                                                                  num_features)
    if target_data is None:
        for batch in range(num_batches):
            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]
        return output_data
    else:
        output_targets = np.repeat(np.nan, repeats = num_batches)
        for batch in range(num_batches):
            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]
            output_targets[batch] = target_data[(shift*batch + (window_length-1))]
        return output_data, output_targets

"""## Vérifiez si les fonctions fonctionnent comme prévu ou non"""

data = np.array([[1,2,3,4,5],
                 [6,7,8,9,10],
                 [11,12,13,14,15],
                 [16,17,18,19,20],
                 [21,22,23,24,25],
                 [26,27,28,29,30]])
ruls = np.array([31,32,33,34,35,36])
print("Data:")
print(data)
print()
print("RUL:")
print(ruls)

check_data, check_rul = process_input_data_with_targets(data,target_data = ruls, window_length = 3, shift = 1)
print(check_data)
print()
print(check_rul)

"""La fonction a divisé les données d'origine en fenêtres de taille 3 avec un chevauchement d'une ligne (décalage de 1) pour structurer les données pour les entraîner

### on va exécuté maintenant la fonction process_input_data_with_targets avec les paramètres window_length = 3 et shift = 2
"""

check_data_2, check_rul_2 = process_input_data_with_targets(data,target_data = ruls, window_length = 3, shift = 2)
print(check_data_2)
print()
print(check_rul_2)

"""### comment la fonction process_input_data_with_targets peut être utilisée pour prétraiter les données réelles du jeu de données CMAPSS ?"""

check_data_new = process_input_data_with_targets(data, target_data = None, window_length = 4, shift = 1)
check_data_new

"""**Utilisation de ces fonctions
Ces fonctions sont maintenant prêtes à être utilisées pour prétraiter les données du jeu de données CMAPSS en vue d'entraîner des modèles de prédiction du RUL. **

# Comment prétraiter les données de test ?

La fonction process_test_data offre la flexibilité nécessaire pour ajuster le nombre de fenêtres utilisées en fonction de la situation spécifique
"""

def process_test_data(test_data_for_an_engine, window_length, shift, num_test_windows = 1):
    """
    This function takes test data for an engine as first input. The next two inputs,
    window_length and shift are same as other functions.

    Finally it takes num_test_windows as the last input. num_test_windows sets how many examples we
    want from test data (from last). By default it extracts only the last example.

    The function returns last examples and number of last examples (a scaler) as output.
    We need the second output later. If we are extracting more than 1 last examples, we have to
    average their prediction results. The second scaler helps us do just that.

    Arguments:
        test_data_for_an_engine: (2D array) input test data
        window_length: (scalar) window length of data
        shift: (scalar) Distance by which the window moves for next batch. This is closely related to overlap
               between data. For example, if window length is 30 and shift is 1, there is an overlap of
               29 data points between two consecutive batches.
        num_test_windows: (scalar) Number of examples to take from last. (default: 1)

    Returns:
        batched_test_data_for_an_engine: (2D array) Batched test data. Please note that, here we have used the term `batch`
                                         in a different sense than its usual meaning in deep learning.
        extracted_num_test_windows: (scalar) Extracted number of test examples from last. If `num_test_windows` number of
                                    examples can't be extracted from test data, in that case `extracted_num_test_windows`
                                    is less than `num_test_windows`.
    """
    max_num_test_batches = int(np.floor((len(test_data_for_an_engine) - window_length)/shift)) + 1
    if max_num_test_batches < num_test_windows:
        required_len = (max_num_test_batches -1)* shift + window_length
        batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],
                                                                          target_data = None,
                                                                          window_length = window_length, shift = shift)
        extracted_num_test_windows = max_num_test_batches
        return batched_test_data_for_an_engine, extracted_num_test_windows
    else:
        required_len = (num_test_windows - 1) * shift + window_length
        batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],
                                                                          target_data = None,
                                                                          window_length = window_length, shift = shift)
        extracted_num_test_windows = num_test_windows
        return batched_test_data_for_an_engine, extracted_num_test_windows

import numpy as np
check_data = np.reshape(np.arange(24), newshape = (6,4))
check_data

last_examples, num_last_examples = process_test_data(check_data, window_length = 2, shift = 1, num_test_windows= 1)
print(last_examples)
print()
print(num_last_examples)

"""last_examples: Contient la dernière fenêtre extraite.
num_last_examples: Retourne 1, confirmant que seulement une fenêtre de données a été extraite.
"""

last_examples, num_last_examples = process_test_data(check_data, window_length = 2, shift = 1, num_test_windows= 3)
print(last_examples)
print()
print(num_last_examples)

"""last_examples : Contient les 3 dernières fenêtres de données, chacune de taille (2, 4) (2 lignes, 4 colonnes).
num_last_examples : Valeur 3, indiquant que 3 fenêtres ont été extraites.
"""

last_examples, num_last_examples = process_test_data(check_data, window_length = 1, shift = 1, num_test_windows= 3)
print(last_examples)
print()
print(num_last_examples)

"""last_examples : Contient les 3 dernières lignes des données, chacune sous forme de fenêtre de taille (1, 4).
num_last_examples : Valeur 3, indiquant que 3 fenêtres ont été extraites.

### Comment faire évoluer les données ?

Pour certains algorithmes, il est nécessaire de mettre à l'échelle les données avant de les utiliser. Dans ce code, on utilise des scalers intégrés de scikit-learn, comme MinMaxScaler, pour normaliser les données. cela montre comment prétraiter à la fois les données d'entraînement et de test avec cette mise à l'échelle.
"""

from sklearn.preprocessing import MinMaxScaler
window_length = 30
shift = 1
early_rul = 125             # Set to None for linear degradation model
processed_train_data = []
processed_train_targets = []

# How many test examples to take for each engine. If set to 1 (this is the default), only last example of test data for
# each engine are taken. If set to a different number, that many examples from last are taken.
# Final output for an engine will be the average of output of all examples for that engine.
num_test_windows = 1     # Number of examples. Change to a different number to select that many test examples for each engine.
processed_test_data = []
num_test_windows_list = []  # This list keeps track of number of examples for all engines in the test set.

num_machines = np.min([len(train_data[0].unique()), len(test_data[0].unique())])

for i in np.arange(1, num_machines + 1):

    temp_train_data = train_data[train_data[0] == i].drop(columns=[0,1,2,3,4,5,9,10,14,20,22,23]).values
    temp_test_data = test_data[test_data[0] == i].drop(columns=[0,1,2,3,4,5,9,10,14,20,22,23]).values

    # Verify if data of given window length can be extracted from both training and test data
    if (len(temp_test_data) < window_length):
        print("Test engine {} doesn't have enough data for window_length of {}".format(i, window_length))
        raise AssertionError("Window length is larger than number of data points for some engines. "
                             "Try decreasing window length.")
    elif (len(temp_train_data) < window_length):
        print("Train engine {} doesn't have enough data for window_length of {}".format(i, window_length))
        raise AssertionError("Window length is larger than number of data points for some engines. "
                             "Try decreasing window length.")

    # We have used MinMaxScaler. Use a different scaler if you want.
    # Or just comment the following 3 lines if no scaling is required.
    scaler = MinMaxScaler(feature_range = (-1, 1))
    temp_train_data = scaler.fit_transform(temp_train_data)
    temp_test_data = scaler.transform(temp_test_data)

    temp_train_targets = process_targets(data_length = temp_train_data.shape[0], early_rul = early_rul)
    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(temp_train_data, temp_train_targets,
                                                                                window_length = window_length, shift = shift)

    # Prepare test data
    test_data_for_an_engine, num_windows = process_test_data(temp_test_data, window_length = window_length, shift = shift,
                                                             num_test_windows = num_test_windows)

    processed_train_data.append(data_for_a_machine)
    processed_train_targets.append(targets_for_a_machine)

    processed_test_data.append(test_data_for_an_engine)
    num_test_windows_list.append(num_windows)

processed_train_data = np.concatenate(processed_train_data)
processed_train_targets = np.concatenate(processed_train_targets)
processed_test_data = np.concatenate(processed_test_data)
true_rul = true_rul[0].values

# Shuffle data
index = np.random.permutation(len(processed_train_targets))
processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]

"""Cette préparation est cruciale pour garantir que les algorithmes de machine learning reçoivent des données de qualité et bien normalisées."""

print(processed_train_data.shape)
print(processed_train_targets.shape)
print(processed_test_data.shape)
print(true_rul.shape)

"""cela imprime les formes des tableaux de données que nous avons préparés pour l'entraînement et les tests, ainsi que les cibles et les vérités réelles (true RUL)

### Comment mettre à l'échelle l'ensemble des données ?
"""

from sklearn.preprocessing import StandardScaler
train_data = pd.read_csv("train_FD001.txt", sep= "\s+", header = None)
test_data = pd.read_csv("test_FD001.txt", sep = "\s+", header = None)
true_rul = pd.read_csv("RUL_FD001.txt", sep = '\s+', header = None)

window_length = 30
shift = 1
early_rul = 125
processed_train_data = []
processed_train_targets = []

# How many test windows to take for each engine. If set to 1 (this is the default), only last window of test data for
# each engine is taken. If set to a different number, that many windows from last are taken.
# Final output is the average output of all windows.
num_test_windows = 1
processed_test_data = []
num_test_windows_list = []

columns_to_be_dropped = [0,1,2,3,4,5,9,10,14,20,22,23]

train_data_first_column = train_data[0]
test_data_first_column = test_data[0]

# Scale data for all engines
scaler = StandardScaler()
train_data = scaler.fit_transform(train_data.drop(columns = columns_to_be_dropped))
test_data = scaler.transform(test_data.drop(columns = columns_to_be_dropped))

train_data = pd.DataFrame(data = np.c_[train_data_first_column, train_data])
test_data = pd.DataFrame(data = np.c_[test_data_first_column, test_data])

num_train_machines = len(train_data[0].unique())
num_test_machines = len(test_data[0].unique())

# Process training and test data separately as number of engines in training and test set may be different.
# As we are doing scaling for full dataset, we are not bothered by different number of engines in training and test set.

# Process training data
for i in np.arange(1, num_train_machines + 1):
    temp_train_data = train_data[train_data[0] == i].drop(columns = [0]).values

    # Verify if data of given window length can be extracted from training data
    if (len(temp_train_data) < window_length):
        print("Train engine {} doesn't have enough data for window_length of {}".format(i, window_length))
        raise AssertionError("Window length is larger than number of data points for some engines. "
                             "Try decreasing window length.")

    temp_train_targets = process_targets(data_length = temp_train_data.shape[0], early_rul = early_rul)
    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(temp_train_data, temp_train_targets,
                                                                                window_length = window_length, shift = shift)

    processed_train_data.append(data_for_a_machine)
    processed_train_targets.append(targets_for_a_machine)

processed_train_data = np.concatenate(processed_train_data)
processed_train_targets = np.concatenate(processed_train_targets)

# Process test data
for i in np.arange(1, num_test_machines + 1):
    temp_test_data = test_data[test_data[0] == i].drop(columns = [0]).values

    # Verify if data of given window length can be extracted from test data
    if (len(temp_test_data) < window_length):
        print("Test engine {} doesn't have enough data for window_length of {}".format(i, window_length))
        raise AssertionError("Window length is larger than number of data points for some engines. "
                             "Try decreasing window length.")

    # Prepare test data
    test_data_for_an_engine, num_windows = process_test_data(temp_test_data, window_length = window_length, shift = shift,
                                                             num_test_windows = num_test_windows)

    processed_test_data.append(test_data_for_an_engine)
    num_test_windows_list.append(num_windows)

processed_test_data = np.concatenate(processed_test_data)
true_rul = true_rul[0].values

# Shuffle training data
index = np.random.permutation(len(processed_train_targets))
processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]

print("Processed training data shape: ", processed_train_data.shape)
print("Processed training rul shape: ", processed_train_targets.shape)
print("Processed test data shape: ", processed_test_data.shape)
print("True RUL shape: ", true_rul.shape)

"""# ***Tests avec emulation***"""

pip install numpy pandas scikit-learn tensorflow

print(train_data.columns)

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.models import Model

# Charger les données
def load_data(file_path):
    data = pd.read_csv(file_path, delim_whitespace=True, header=None)
    data.columns = ['engine_id', 'cycle', 'setting_1', 'setting_2', 'setting_3', 'sensor_1',
                     'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7',
                     'sensor_8', 'sensor_9', 'sensor_10', 'sensor_11', 'sensor_12',
                     'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16', 'sensor_17',
                     'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21']
    return data

# Normaliser les données
def normalize_data(data, sensor_columns):
    scaler = StandardScaler()
    data[sensor_columns] = scaler.fit_transform(data[sensor_columns])
    return data, scaler

# Préparer les données
def prepare_data(data, sensor_columns, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data.iloc[i:i+sequence_length, 2:].values)
        y.append(data.iloc[i+sequence_length, 1])
    return np.array(X), np.array(y)

# Créer le modèle
def create_model(input_shape):
    inputs = Input(shape=input_shape)
    x = LSTM(100, return_sequences=True)(inputs)
    x = LSTM(50, return_sequences=False)(x)
    outputs = Dense(1)(x)
    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='mse')
    return model

# Charger les données
sensor_columns = [f'sensor_{i}' for i in range(1, 22)]
enriched_train_data = pd.read_csv('enriched_train_FD001.csv')
test_data = load_data('test_FD001.txt')

# Normaliser les données
enriched_train_data, scaler = normalize_data(enriched_train_data, sensor_columns)
test_data, _ = normalize_data(test_data, sensor_columns)

# Préparer les données
sequence_length = 30
X_train, y_train = prepare_data(enriched_train_data, sensor_columns, sequence_length)
X_test, y_test = prepare_data(test_data, sensor_columns, sequence_length)

# Créer et entraîner le modèle
input_shape = (X_train.shape[1], X_train.shape[2])
model = create_model(input_shape)
model.fit(X_train, y_train, epochs=10, batch_size=64)

# Prédire le RUL sur les données de test
y_pred = model.predict(X_test)

# Évaluer le modèle
def evaluate_model(y_true, y_pred):
    mse = np.mean((y_true - y_pred.squeeze())**2)
    print(f"Mean Squared Error (MSE): {mse}")

evaluate_model(y_test, y_pred)

# Émuler des scénarios
def emulate_scenarios(test_data, sensor_columns, scenario='random_noise'):
    if scenario == 'random_noise':
        noisy_data = test_data.copy()
        noise_factor = 0.1
        for col in sensor_columns:
            noisy_data[col] += noise_factor * np.random.normal(size=len(test_data))
        return noisy_data
    elif scenario == 'sensor_failure':
        failed_sensor = np.random.choice(sensor_columns)
        failed_data = test_data.copy()
        failed_data[failed_sensor] = 0
        return failed_data
    return test_data

# Utiliser l'émulation
emulated_test_data = emulate_scenarios(test_data, sensor_columns, scenario='random_noise')

# Préparer les données émulées
emulated_X_test, _ = prepare_data(emulated_test_data, sensor_columns, sequence_length)

# Prédire sur les données émulées
emulated_y_pred = model.predict(emulated_X_test)

# Évaluer le modèle sur les données émulées
evaluate_model(y_test, emulated_y_pred)

# Afficher les résultats
import matplotlib.pyplot as plt

def plot_results(y_true, y_pred, title='RUL Réel vs Prédit'):
    plt.figure(figsize=(14, 7))
    plt.plot(y_true, label='RUL Réel', color='blue')
    plt.plot(y_pred, label='RUL Prédit', color='red', alpha=0.7)
    plt.xlabel('Index')
    plt.ylabel('RUL')
    plt.title(title)
    plt.legend()
    plt.show()

plot_results(y_test, emulated_y_pred, title='RUL Réel vs Prédit pour Scénarios Émulés')

"""## entrainement avec algo de PSO

"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

# Charger les données
def load_data(file_path):
    data = pd.read_csv(file_path, delim_whitespace=True, header=None)
    data.columns = ['engine_id', 'cycle', 'setting_1', 'setting_2', 'setting_3', 'sensor_1',
                     'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7',
                     'sensor_8', 'sensor_9', 'sensor_10', 'sensor_11', 'sensor_12',
                     'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16', 'sensor_17',
                     'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21']
    return data

# Normaliser les données
def normalize_data(data, sensor_columns):
    scaler = StandardScaler()
    data[sensor_columns] = scaler.fit_transform(data[sensor_columns])
    return data, scaler

# Préparer les données
def prepare_data(data, sensor_columns, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data.iloc[i:i+sequence_length, 2:].values)
        y.append(data.iloc[i+sequence_length, 1])
    return np.array(X), np.array(y)

# Créer le modèle avec les paramètres de PSO
def create_model(input_shape, lstm_units_1, lstm_units_2, learning_rate):
    inputs = Input(shape=input_shape)
    x = LSTM(lstm_units_1, return_sequences=True)(inputs)
    x = LSTM(lstm_units_2, return_sequences=False)(x)
    outputs = Dense(1)(x)
    model = Model(inputs, outputs)
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    return model

# Fonction d'évaluation pour PSO
def objective(params):
    lstm_units_1, lstm_units_2, learning_rate = int(params[0]), int(params[1]), params[2]
    model = create_model(input_shape, lstm_units_1, lstm_units_2, learning_rate)
    model.fit(X_train, y_train, epochs=3, batch_size=64, verbose=0)
    y_pred = model.predict(X_test)
    mse = np.mean((y_test - y_pred.squeeze())**2)
    return mse

# Implémentation simplifiée de PSO
def pso_optimize(objective_func, bounds, num_particles=10, max_iter=5):
    num_params = len(bounds)
    # Initialiser les positions et vitesses
    particles = np.random.rand(num_particles, num_params) * (bounds[:,1] - bounds[:,0]) + bounds[:,0]
    velocities = np.random.rand(num_particles, num_params)
    personal_best_positions = particles.copy()
    personal_best_scores = np.array([objective_func(p) for p in particles])
    global_best_position = personal_best_positions[personal_best_scores.argmin()]
    global_best_score = personal_best_scores.min()

    for _ in range(max_iter):
        for i in range(num_particles):
            # Mise à jour de la vitesse et position
            r1, r2 = np.random.rand(num_params), np.random.rand(num_params)
            velocities[i] = (0.5 * velocities[i] +
                             0.5 * r1 * (personal_best_positions[i] - particles[i]) +
                             0.5 * r2 * (global_best_position - particles[i]))
            particles[i] += velocities[i]
            particles[i] = np.clip(particles[i], bounds[:,0], bounds[:,1])

            # Mise à jour des scores
            score = objective_func(particles[i])
            if score < personal_best_scores[i]:
                personal_best_scores[i] = score
                personal_best_positions[i] = particles[i]
            if score < global_best_score:
                global_best_score = score
                global_best_position = particles[i]

    return global_best_position, global_best_score

# Charger et normaliser les données
sensor_columns = [f'sensor_{i}' for i in range(1, 22)]
enriched_train_data = pd.read_csv('enriched_train_FD001.csv')
test_data = load_data('test_FD001.txt')

enriched_train_data, scaler = normalize_data(enriched_train_data, sensor_columns)
test_data, _ = normalize_data(test_data, sensor_columns)

# Préparer les données
sequence_length = 30
X_train, y_train = prepare_data(enriched_train_data, sensor_columns, sequence_length)
X_test, y_test = prepare_data(test_data, sensor_columns, sequence_length)
input_shape = (X_train.shape[1], X_train.shape[2])

# Limites pour les hyperparamètres à optimiser
bounds = np.array([[50, 150], [20, 100], [0.0001, 0.01]])

# Exécuter PSO pour trouver les meilleurs paramètres
best_params, best_mse = pso_optimize(objective, bounds)
print(f"Best params found by PSO: {best_params}")
print(f"Best MSE found by PSO: {best_mse}")

# Entraîner le modèle final avec les meilleurs paramètres
lstm_units_1, lstm_units_2, learning_rate = int(best_params[0]), int(best_params[1]), best_params[2]
model = create_model(input_shape, lstm_units_1, lstm_units_2, learning_rate)
model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)

# Prédire le RUL et évaluer
y_pred = model.predict(X_test)
def evaluate_model(y_true, y_pred):
    mse = np.mean((y_true - y_pred.squeeze())**2)
    print(f"Mean Squared Error (MSE): {mse}")

evaluate_model(y_test, y_pred)

# Afficher les résultats
def plot_results(y_true, y_pred, title='RUL Réel vs Prédit'):
    plt.figure(figsize=(14, 7))
    plt.plot(y_true, label='RUL Réel', color='blue')
    plt.plot(y_pred, label='RUL Prédit', color='red', alpha=0.7)
    plt.xlabel('Index')
    plt.ylabel('RUL')
    plt.title(title)
    plt.legend()
    plt.show()

plot_results(y_test, y_pred, title='RUL Réel vs Prédit après PSO')

"""## creer et Sauvegarder fichier qui contient les Valeurs Prédites et relles dans un Fichier CSV

# utiliser Streamlit pour créer l interface
"""

!pip install streamlit

import pandas as pd

# Charger le fichier texte contenant les RUL réels
rul_reel = pd.read_csv('RUL_FD001.txt', header=None)
rul_reel.columns = ['RUL_Réel']

# Prédictions des RUL par le modèle (exemple, à remplacer par vos vraies prédictions)
# y_pred est le tableau numpy contenant les RUL prédites par votre modèle
# Assurez-vous que y_pred a la même taille que rul_reel
rul_predit = pd.DataFrame(y_pred, columns=['RUL_Prédit'])

# Combiner les RUL réels et prédites dans un seul DataFrame
rul_combine = pd.concat([rul_reel, rul_predit], axis=1)

# Sauvegarder le DataFrame combiné dans un fichier CSV
rul_combine.to_csv('rul_combine.csv', index=False)

print("Fichier CSV créé avec succès : rul_combine.csv")

from google.colab import files

# Télécharger le fichier CSV généré
files.download('rul_combine.csv')